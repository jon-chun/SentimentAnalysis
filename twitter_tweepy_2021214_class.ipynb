{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jon-chun/SentimentAnalysis/blob/master/twitter_tweepy_2021214_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OEfF-bH0gNs"
      },
      "source": [
        "# **Sentiment Analysis of Tweets**\n",
        "\n",
        "Jon Chun\n",
        "14 Oct 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXhExu2hf3rz"
      },
      "source": [
        "# **Install Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z3yUXnvn42U"
      },
      "outputs": [],
      "source": [
        "# Library to clean text\n",
        "\n",
        "!pip install texthero\n",
        "\n",
        "# Other text cleaning libraries\n",
        "\n",
        "# !pip install clean-text (better)\n",
        "\n",
        "# !pip install tweet-preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXSHBs1drmmy"
      },
      "outputs": [],
      "source": [
        "# Library to automate twitter OAuth/scraping\n",
        "\n",
        "!pip install tweepy\n",
        "\n",
        "# twint (better when working, no API limitations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrM06ND6xQfv"
      },
      "outputs": [],
      "source": [
        "# NLP cleaning\n",
        "\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc5u3RIGjYR7"
      },
      "outputs": [],
      "source": [
        "# Translate emojis and emoticons to text\n",
        "\n",
        "!pip install emot\n",
        "\n",
        "# !pip install emoji\n",
        "\n",
        "# !pip install demoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKxFnd_TqAjg"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qafMPRKfTwVl"
      },
      "source": [
        "## Spell Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOsG_gIzJ_JN"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get update -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sFm4af-KDJ1"
      },
      "outputs": [],
      "source": [
        "# !sudo apt-get install -y swig3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoZnJlfIH3m_"
      },
      "outputs": [],
      "source": [
        "# !pip install jamspell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EILBo1v4Tii1"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rj6sROVr0-k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tweepy\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGtalmAYn7a-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# import string\n",
        "\n",
        "import texthero as hero\n",
        "from texthero import preprocessing\n",
        "\n",
        "import contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpU28GyPqBPQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from cleantext import clean\n",
        "\n",
        "clean(\"some input\",\n",
        "    fix_unicode=True,               # fix various unicode errors\n",
        "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "    lower=True,                     # lowercase text\n",
        "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
        "    no_urls=False,                  # replace all URLs with a special token\n",
        "    no_emails=False,                # replace all email addresses with a special token\n",
        "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
        "    no_numbers=False,               # replace all numbers with a special token\n",
        "    no_digits=False,                # replace all digits with a special token\n",
        "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
        "    no_punct=False,                 # remove punctuations\n",
        "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "    replace_with_url=\"<URL>\",\n",
        "    replace_with_email=\"<EMAIL>\",\n",
        "    replace_with_phone_number=\"<PHONE>\",\n",
        "    replace_with_number=\"<NUMBER>\",\n",
        "    replace_with_digit=\"0\",\n",
        "    replace_with_currency_symbol=\"<CUR>\",\n",
        "    lang=\"en\"                       # set to 'de' for German special handling\n",
        ")\n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PBsG4WRMvrN"
      },
      "outputs": [],
      "source": [
        "import emot \n",
        "emot_obj = emot.core.emot() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxsBj2AEMvia"
      },
      "outputs": [],
      "source": [
        "# Test emot\n",
        "\n",
        "text = \"I love python ‚òÆ üôÇ ‚ù§ :-) :-( :-)))\" \n",
        "emot_obj.emoticons(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0WLecuQjX6a"
      },
      "outputs": [],
      "source": [
        "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
        "\n",
        "# import emoji\n",
        "# import demoji\n",
        "# demoji.download_codes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5r56WvooLfY"
      },
      "outputs": [],
      "source": [
        "# import preprocessor as tweetproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYit2wRUqGa3"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "vader_sa = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMt76YLu56H6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rzle3u3UpIx"
      },
      "source": [
        "# **Configuration**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMDR87ZN1pFq"
      },
      "outputs": [],
      "source": [
        "#collapse_show\n",
        "pd.set_option('display.max_colwidth', -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycraKQUK0Uzj"
      },
      "outputs": [],
      "source": [
        "# Enlarge matplotlib plots\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVkioZCgf9Eq"
      },
      "source": [
        "# **Globals**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZBLqffVf-x6"
      },
      "outputs": [],
      "source": [
        "# Dictionary of DataFrames (key=search_term) \n",
        "tweets_searchterm_dt = {}\n",
        "\n",
        "# Dictionary of DataFrames (key=username)\n",
        "tweets_user_dt = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffITIez13tmD"
      },
      "source": [
        "# **Authenticate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nt8Gn496Hwq"
      },
      "source": [
        "## **Google gDrive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY1SMpvL6Hkr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCyknvKU6Q7j"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP3RkutT6SOM"
      },
      "outputs": [],
      "source": [
        "%cd ./MyDrive/courses/senior_projects/fall2021/nlp_twitter_covid/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNY_lBL56Jox"
      },
      "source": [
        "## **Twitter OAuth**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-d-o4McKqb9"
      },
      "outputs": [],
      "source": [
        "#collapse_hide\n",
        "#for privacy purposes, these keys are encrypted\n",
        "consumer_key=\"s25Oa7fnvICpEpHCF6VDcatoq\"\n",
        "consumer_secret=\"jgXTDIFXfIaNI6DKPZX3U9amuRr8w2z4OH84yZSUBCic6kVdsz\"\n",
        "access_token=\"1297985939831173120-cP4xtUtJXtSH6WMBUNFbMBWxgUmA5P\"\n",
        "access_token_secret=\"5cSrDyCyaqvqd6LXiXPsPyAygT8Mt9RjvHjPkFV6eREOV\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io1bPK-n1mJH"
      },
      "outputs": [],
      "source": [
        "#collapse_show\n",
        "#Accessing twitter API\n",
        "auth=tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token=(access_token, access_token_secret)\n",
        "api= tweepy.API(auth, wait_on_rate_limit= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTxaxmNvjgeI"
      },
      "outputs": [],
      "source": [
        "# Test tweepy OAuth\n",
        "\n",
        "tweets= tweepy.Cursor(api.user_timeline,id= 'joebiden', tweet_mode=\"extended\").items(5)\n",
        "\n",
        "tweet_ls = [tweet for tweet in tweets]\n",
        "\n",
        "tweet_ls[0].id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxG8WkoEjgW9"
      },
      "outputs": [],
      "source": [
        "# dir(tweet_ls[0])\n",
        "\n",
        "tweet_ls[0].full_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHTnyhPDTpgj"
      },
      "source": [
        "# **Utility Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaNCz04rVYui"
      },
      "source": [
        "## Clean Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ATVg6Qfkftl"
      },
      "outputs": [],
      "source": [
        "type(EMOTICONS_EMO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qttKqEqFkc7v"
      },
      "outputs": [],
      "source": [
        "# UNICODE_EMOJI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPBRXkOQjrCH"
      },
      "outputs": [],
      "source": [
        "def emojis2text(atext):\n",
        "  for emot, text_desc in UNICODE_EMOJI.items():\n",
        "    atext = atext.replace(emot, ' '.join(text_desc.replace(\",\", \"\").split()))\n",
        "\n",
        "  # atext = re.sub(r':([^:]*):',r'\\1',atext)\n",
        "  atext = atext.replace('_', ' ').replace(':','')\n",
        "  # atext2 = atext.replace(':', '')\n",
        "\n",
        "  return atext\n",
        "\n",
        "# Test\n",
        "test_str = \"Hilarious üòÇ. The feeling of making a sale üòé, The feeling of actually ;) fulfilling orders üòí\"\n",
        "test_str = emojis2text(test_str)\n",
        "print(f'test_str: [{test_str}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6WJ45RdlK7S"
      },
      "outputs": [],
      "source": [
        "def emoticons2text(atext):\n",
        "  for emot, text_desc in EMOTICONS_EMO.items():\n",
        "    atext = atext.replace(emot, ' '.join(text_desc.replace(\",\", \"\").split()))\n",
        "  return atext\n",
        "\n",
        "# Test\n",
        "test_str = \"Hilarious :o. The feeling of making a sale :( , The feeling of actually ;) fulfilling orders üòí\"\n",
        "test_str = emoticons2text(test_str)\n",
        "print(f'test_str: [{test_str}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTDLMAZAm8dw"
      },
      "outputs": [],
      "source": [
        "def all_emos2text(atext):\n",
        "  '''\n",
        "  Given a text string with embedded emojis and/or emoticons\n",
        "  Return a expanded text string with all emojis/emoticons translated into text\n",
        "  '''\n",
        "\n",
        "  # First, convert emoticons to text\n",
        "  for emot, text_desc in EMOTICONS_EMO.items():\n",
        "    atext = atext.replace(emot, ' ' + ' '.join(text_desc.replace(\",\", \" \").split()))\n",
        "\n",
        "  # Second, convert emojis to text\n",
        "  for emot, text_desc in UNICODE_EMOJI.items():\n",
        "    atext = atext.replace(emot, ' ' + ' '.join(text_desc.replace(\",\", \" \").split()))\n",
        "\n",
        "  atext = re.sub(r':([A-Za-z_]*):',r'\\1',atext)\n",
        "  # atext = re.sub(r'([\\w]+)([_])([\\w]+)',r'\\1 \\3',atext)\n",
        "  atext = re.sub(r'_', ' ', atext)\n",
        "\n",
        "  return atext\n",
        "\n",
        "# Test\n",
        "test_str = \"Hilarious üòÇ. The feeling :o of making a sale üòé, The feeling :( of actually ;) fulfilling orders üòí\"\n",
        "all_emos2text(test_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa3I1KCockio"
      },
      "outputs": [],
      "source": [
        "hero.preprocessing.get_default_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ANrdRjYceeX"
      },
      "outputs": [],
      "source": [
        "from texthero import preprocessing\n",
        "\n",
        "hero_pre_pipeline =  [preprocessing.remove_urls,\n",
        "                      preprocessing.remove_html_tags]\n",
        "\n",
        "hero_post_pipeline = [preprocessing.fillna,\n",
        "                      preprocessing.lowercase,\n",
        "                      preprocessing.remove_digits,\n",
        "                      preprocessing.remove_punctuation,\n",
        "                      preprocessing.remove_diacritics,\n",
        "                      preprocessing.remove_stopwords,\n",
        "                      preprocessing.remove_whitespace]\n",
        "\n",
        "# df['clean_text'] = hero.clean(df['text'], hero_pipeline)\n",
        "# or\n",
        "# df['clean_text'] = df['clean_text'].pipe(hero.clean, custom_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw84Hduib7J7"
      },
      "source": [
        "### [Optional] Slang Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EmymCL2KV3a"
      },
      "outputs": [],
      "source": [
        "# Abbreviation / Slang\n",
        "# https://www.kaggle.com/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing/notebook\n",
        "\n",
        "slang = {\n",
        "    \"$\" : \" dollar \",\n",
        "    \"‚Ç¨\" : \" euro \",\n",
        "    \"4ao\" : \"for adults only\",\n",
        "    \"a.m\" : \"before midday\",\n",
        "    \"a3\" : \"anytime anywhere anyplace\",\n",
        "    \"aamof\" : \"as a matter of fact\",\n",
        "    \"acct\" : \"account\",\n",
        "    \"adih\" : \"another day in hell\",\n",
        "    \"afaic\" : \"as far as i am concerned\",\n",
        "    \"afaict\" : \"as far as i can tell\",\n",
        "    \"afaik\" : \"as far as i know\",\n",
        "    \"afair\" : \"as far as i remember\",\n",
        "    \"afk\" : \"away from keyboard\",\n",
        "    \"app\" : \"application\",\n",
        "    \"approx\" : \"approximately\",\n",
        "    \"apps\" : \"applications\",\n",
        "    \"asap\" : \"as soon as possible\",\n",
        "    \"asl\" : \"age, sex, location\",\n",
        "    \"atk\" : \"at the keyboard\",\n",
        "    \"ave.\" : \"avenue\",\n",
        "    \"aymm\" : \"are you my mother\",\n",
        "    \"ayor\" : \"at your own risk\", \n",
        "    \"b&b\" : \"bed and breakfast\",\n",
        "    \"b+b\" : \"bed and breakfast\",\n",
        "    \"b.c\" : \"before christ\",\n",
        "    \"b2b\" : \"business to business\",\n",
        "    \"b2c\" : \"business to customer\",\n",
        "    \"b4\" : \"before\",\n",
        "    \"b4n\" : \"bye for now\",\n",
        "    \"b@u\" : \"back at you\",\n",
        "    \"bae\" : \"before anyone else\",\n",
        "    \"bak\" : \"back at keyboard\",\n",
        "    \"bbbg\" : \"bye bye be good\",\n",
        "    \"bbc\" : \"british broadcasting corporation\",\n",
        "    \"bbias\" : \"be back in a second\",\n",
        "    \"bbl\" : \"be back later\",\n",
        "    \"bbs\" : \"be back soon\",\n",
        "    \"be4\" : \"before\",\n",
        "    \"bfn\" : \"bye for now\",\n",
        "    \"blvd\" : \"boulevard\",\n",
        "    \"bout\" : \"about\",\n",
        "    \"brb\" : \"be right back\",\n",
        "    \"bros\" : \"brothers\",\n",
        "    \"brt\" : \"be right there\",\n",
        "    \"bsaaw\" : \"big smile and a wink\",\n",
        "    \"btw\" : \"by the way\",\n",
        "    \"bwl\" : \"bursting with laughter\",\n",
        "    \"c/o\" : \"care of\",\n",
        "    \"cet\" : \"central european time\",\n",
        "    \"cf\" : \"compare\",\n",
        "    \"cia\" : \"central intelligence agency\",\n",
        "    \"csl\" : \"can not stop laughing\",\n",
        "    \"cu\" : \"see you\",\n",
        "    \"cul8r\" : \"see you later\",\n",
        "    \"cv\" : \"curriculum vitae\",\n",
        "    \"cwot\" : \"complete waste of time\",\n",
        "    \"cya\" : \"see you\",\n",
        "    \"cyt\" : \"see you tomorrow\",\n",
        "    \"dae\" : \"does anyone else\",\n",
        "    \"dbmib\" : \"do not bother me i am busy\",\n",
        "    \"diy\" : \"do it yourself\",\n",
        "    \"dm\" : \"direct message\",\n",
        "    \"dwh\" : \"during work hours\",\n",
        "    \"e123\" : \"easy as one two three\",\n",
        "    \"eet\" : \"eastern european time\",\n",
        "    \"eg\" : \"example\",\n",
        "    \"embm\" : \"early morning business meeting\",\n",
        "    \"encl\" : \"enclosed\",\n",
        "    \"encl.\" : \"enclosed\",\n",
        "    \"etc\" : \"and so on\",\n",
        "    \"faq\" : \"frequently asked questions\",\n",
        "    \"fawc\" : \"for anyone who cares\",\n",
        "    \"fb\" : \"facebook\",\n",
        "    \"fc\" : \"fingers crossed\",\n",
        "    \"fig\" : \"figure\",\n",
        "    \"fimh\" : \"forever in my heart\", \n",
        "    \"ft.\" : \"feet\",\n",
        "    \"ft\" : \"featuring\",\n",
        "    \"ftl\" : \"for the loss\",\n",
        "    \"ftw\" : \"for the win\",\n",
        "    \"fwiw\" : \"for what it is worth\",\n",
        "    \"fyi\" : \"for your information\",\n",
        "    \"g9\" : \"genius\",\n",
        "    \"gahoy\" : \"get a hold of yourself\",\n",
        "    \"gal\" : \"get a life\",\n",
        "    \"gcse\" : \"general certificate of secondary education\",\n",
        "    \"gfn\" : \"gone for now\",\n",
        "    \"gg\" : \"good game\",\n",
        "    \"gl\" : \"good luck\",\n",
        "    \"glhf\" : \"good luck have fun\",\n",
        "    \"gmt\" : \"greenwich mean time\",\n",
        "    \"gmta\" : \"great minds think alike\",\n",
        "    \"gn\" : \"good night\",\n",
        "    \"g.o.a.t\" : \"greatest of all time\",\n",
        "    \"goat\" : \"greatest of all time\",\n",
        "    \"goi\" : \"get over it\",\n",
        "    \"gps\" : \"global positioning system\",\n",
        "    \"gr8\" : \"great\",\n",
        "    \"gratz\" : \"congratulations\",\n",
        "    \"gyal\" : \"girl\",\n",
        "    \"h&c\" : \"hot and cold\",\n",
        "    \"hp\" : \"horsepower\",\n",
        "    \"hr\" : \"hour\",\n",
        "    \"hrh\" : \"his royal highness\",\n",
        "    \"ht\" : \"height\",\n",
        "    \"ibrb\" : \"i will be right back\",\n",
        "    \"ic\" : \"i see\",\n",
        "    \"icq\" : \"i seek you\",\n",
        "    \"icymi\" : \"in case you missed it\",\n",
        "    \"idc\" : \"i do not care\",\n",
        "    \"idgadf\" : \"i do not give a damn fuck\",\n",
        "    \"idgaf\" : \"i do not give a fuck\",\n",
        "    \"idk\" : \"i do not know\",\n",
        "    \"ie\" : \"that is\",\n",
        "    \"i.e\" : \"that is\",\n",
        "    \"ifyp\" : \"i feel your pain\",\n",
        "    \"IG\" : \"instagram\",\n",
        "    \"iirc\" : \"if i remember correctly\",\n",
        "    \"ilu\" : \"i love you\",\n",
        "    \"ily\" : \"i love you\",\n",
        "    \"imho\" : \"in my humble opinion\",\n",
        "    \"imo\" : \"in my opinion\",\n",
        "    \"imu\" : \"i miss you\",\n",
        "    \"iow\" : \"in other words\",\n",
        "    \"irl\" : \"in real life\",\n",
        "    \"j4f\" : \"just for fun\",\n",
        "    \"jic\" : \"just in case\",\n",
        "    \"jk\" : \"just kidding\",\n",
        "    \"jsyk\" : \"just so you know\",\n",
        "    \"l8r\" : \"later\",\n",
        "    \"lb\" : \"pound\",\n",
        "    \"lbs\" : \"pounds\",\n",
        "    \"ldr\" : \"long distance relationship\",\n",
        "    \"lmao\" : \"laugh my ass off\",\n",
        "    \"lmfao\" : \"laugh my fucking ass off\",\n",
        "    \"lol\" : \"laughing out loud\",\n",
        "    \"ltd\" : \"limited\",\n",
        "    \"ltns\" : \"long time no see\",\n",
        "    \"m8\" : \"mate\",\n",
        "    \"mf\" : \"motherfucker\",\n",
        "    \"mfs\" : \"motherfuckers\",\n",
        "    \"mfw\" : \"my face when\",\n",
        "    \"mofo\" : \"motherfucker\",\n",
        "    \"mph\" : \"miles per hour\",\n",
        "    \"mr\" : \"mister\",\n",
        "    \"mrw\" : \"my reaction when\",\n",
        "    \"ms\" : \"miss\",\n",
        "    \"mte\" : \"my thoughts exactly\",\n",
        "    \"nagi\" : \"not a good idea\",\n",
        "    \"nbc\" : \"national broadcasting company\",\n",
        "    \"nbd\" : \"not big deal\",\n",
        "    \"nfs\" : \"not for sale\",\n",
        "    \"ngl\" : \"not going to lie\",\n",
        "    \"nhs\" : \"national health service\",\n",
        "    \"nrn\" : \"no reply necessary\",\n",
        "    \"nsfl\" : \"not safe for life\",\n",
        "    \"nsfw\" : \"not safe for work\",\n",
        "    \"nth\" : \"nice to have\",\n",
        "    \"nvr\" : \"never\",\n",
        "    \"nyc\" : \"new york city\",\n",
        "    \"oc\" : \"original content\",\n",
        "    \"og\" : \"original\",\n",
        "    \"ohp\" : \"overhead projector\",\n",
        "    \"oic\" : \"oh i see\",\n",
        "    \"omdb\" : \"over my dead body\",\n",
        "    \"omg\" : \"oh my god\",\n",
        "    \"omw\" : \"on my way\",\n",
        "    \"p.a\" : \"per annum\",\n",
        "    \"p.m\" : \"after midday\",\n",
        "    \"pm\" : \"prime minister\",\n",
        "    \"poc\" : \"people of color\",\n",
        "    \"pov\" : \"point of view\",\n",
        "    \"pp\" : \"pages\",\n",
        "    \"ppl\" : \"people\",\n",
        "    \"prw\" : \"parents are watching\",\n",
        "    \"ps\" : \"postscript\",\n",
        "    \"pt\" : \"point\",\n",
        "    \"ptb\" : \"please text back\",\n",
        "    \"pto\" : \"please turn over\",\n",
        "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
        "    \"ratchet\" : \"rude\",\n",
        "    \"rbtl\" : \"read between the lines\",\n",
        "    \"rlrt\" : \"real life retweet\", \n",
        "    \"rofl\" : \"rolling on the floor laughing\",\n",
        "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\" : \"retweet\",\n",
        "    \"ruok\" : \"are you ok\",\n",
        "    \"sfw\" : \"safe for work\",\n",
        "    \"sk8\" : \"skate\",\n",
        "    \"smh\" : \"shake my head\",\n",
        "    \"sq\" : \"square\",\n",
        "    \"srsly\" : \"seriously\", \n",
        "    \"ssdd\" : \"same stuff different day\",\n",
        "    \"tbh\" : \"to be honest\",\n",
        "    \"tbs\" : \"tablespooful\",\n",
        "    \"tbsp\" : \"tablespooful\",\n",
        "    \"tfw\" : \"that feeling when\",\n",
        "    \"thks\" : \"thank you\",\n",
        "    \"tho\" : \"though\",\n",
        "    \"thx\" : \"thank you\",\n",
        "    \"tia\" : \"thanks in advance\",\n",
        "    \"til\" : \"today i learned\",\n",
        "    \"tl;dr\" : \"too long i did not read\",\n",
        "    \"tldr\" : \"too long i did not read\",\n",
        "    \"tmb\" : \"tweet me back\",\n",
        "    \"tntl\" : \"trying not to laugh\",\n",
        "    \"ttyl\" : \"talk to you later\",\n",
        "    \"u\" : \"you\",\n",
        "    \"u2\" : \"you too\",\n",
        "    \"u4e\" : \"yours for ever\",\n",
        "    \"utc\" : \"coordinated universal time\",\n",
        "    \"w/\" : \"with\",\n",
        "    \"w/o\" : \"without\",\n",
        "    \"w8\" : \"wait\",\n",
        "    \"wassup\" : \"what is up\",\n",
        "    \"wb\" : \"welcome back\",\n",
        "    \"wtf\" : \"what the fuck\",\n",
        "    \"wtg\" : \"way to go\",\n",
        "    \"wtpa\" : \"where the party at\",\n",
        "    \"wuf\" : \"where are you from\",\n",
        "    \"wuzup\" : \"what is up\",\n",
        "    \"wywh\" : \"wish you were here\",\n",
        "    \"yd\" : \"yard\",\n",
        "    \"ygtr\" : \"you got that right\",\n",
        "    \"ynk\" : \"you never know\",\n",
        "    \"zzz\" : \"sleeping bored and tired\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agMcAVpoTITT"
      },
      "outputs": [],
      "source": [
        "def expand_slang(astring):\n",
        "  words_ls = []\n",
        "  words_expanded_ls = []\n",
        "  slang_keys = slang.keys()\n",
        "\n",
        "  words_ls = astring.split()\n",
        "  for aword in words_ls:\n",
        "    if aword.lower() in slang.keys():\n",
        "      words_expanded_ls.append(slang[aword.lower()])\n",
        "    else:\n",
        "      words_expanded_ls.append(aword.lower())\n",
        "\n",
        "  # abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n",
        "\n",
        "  astring_expanded = ' '.join(words_expanded_ls)\n",
        "\n",
        "  return astring_expanded \n",
        "\n",
        "# Test\n",
        "\n",
        "expand_slang('idk LOL you suck!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhYL38H8b1iR"
      },
      "source": [
        "### [Optional] Spell Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McNREyPYI_ku"
      },
      "outputs": [],
      "source": [
        "# import jamspell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QObn0D9LIjV"
      },
      "outputs": [],
      "source": [
        "# !wget https://github.com/bakwc/JamSpell-models/raw/master/en.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-7jsQt0LMyP"
      },
      "outputs": [],
      "source": [
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhY0oq09LN7N"
      },
      "outputs": [],
      "source": [
        "# !gunzip en.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAP0-G6vLXEz"
      },
      "outputs": [],
      "source": [
        "# !tar -xvf en.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx7tXwzxKp92"
      },
      "outputs": [],
      "source": [
        "# corrector = jamspell.TSpellCorrector()\n",
        "# corrector.LoadLangModel('en.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZJ_podyK0Gw"
      },
      "outputs": [],
      "source": [
        "# corrector.FixFragment('I am the begt spell cherken!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtdzKLGebIxb"
      },
      "source": [
        "### Main: clean_tweet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLxwfTA2y49G"
      },
      "outputs": [],
      "source": [
        "from texthero import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpchjKtfy2H4"
      },
      "outputs": [],
      "source": [
        "hero.preprocessing.get_default_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSLKFECpsnlA"
      },
      "outputs": [],
      "source": [
        "# Customize TextHero pipeline\n",
        "\n",
        "# Create a custom cleaning pipeline\n",
        "custom_pipeline = [preprocessing.fillna\n",
        "                   , preprocessing.lowercase\n",
        "                   , preprocessing.remove_digits\n",
        "                   , preprocessing.remove_punctuation\n",
        "                   , preprocessing.remove_diacritics\n",
        "                   , preprocessing.remove_stopwords\n",
        "                   , preprocessing.remove_whitespace\n",
        "                   , preprocessing.stem]\n",
        "                   \n",
        "# Test: pass the custom_pipeline to the pipeline argument\n",
        "# df['clean_title'] = hero.clean(df['title'], pipeline = custom_pipeline)df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj2FduA_aTGd"
      },
      "outputs": [],
      "source": [
        "def clean_tweet(tweet_df, col_text_raw):\n",
        "  '''\n",
        "  Given a DataFrame of Tweets and Column with raw text\n",
        "  Return a Series of Cleaned Tweet texts\n",
        "  '''\n",
        "\n",
        "  text_clean_ser = pd.Series()\n",
        "\n",
        "  # Remove URLs\n",
        "  text_clean_ser = hero.remove_urls(tweet_df[col_text_raw])\n",
        "\n",
        "  # Emoticons and then Emojis to Text\n",
        "  text_clean_ser = text_clean_ser.apply(lambda x : all_emos2text(x))\n",
        "\n",
        "  # Expand Slang/Abbr\n",
        "  text_clean_ser = text_clean_ser.apply(lambda x : expand_slang(x))\n",
        "\n",
        "  # Expand Contractions\n",
        "  text_clean_ser = text_clean_ser.apply(lambda x : contractions.fix(x))\n",
        "\n",
        "  # Clean text: lowercase, remove punctuation/numbers, etc\n",
        "  # text_clean_ser = text_clean_ser.pipe(hero.clean, hero_pre_pipeline)\n",
        "  text_clean_ser = hero.clean(text_clean_ser, pipeline = custom_pipeline)\n",
        "\n",
        "  # Emoji to Text\n",
        "  # text_clean_ser = text_clean_ser.apply(lambda x : emoji.demojize(x))\n",
        "\n",
        "  # Emoticons to Text\n",
        "  # text_clean_ser = text_clean_ser.apply(lambda x : x + ' ' + ' '.join(emot_obj.emoticons(x)['mean']))\n",
        "\n",
        "  # Slang and contractions\n",
        "  # https://www.kaggle.com/rizdelhi/socialmediaabbrevations\n",
        "  # https://www.kaggle.com/longtng/nlp-preprocessing-feature-extraction-methods-a-z\n",
        "  # https://www.kaggle.com/nmaguette/up-to-date-list-of-slangs-for-text-preprocessing\n",
        "  # https://github.com/poddarswakhar/Twitter-Analysis-Abbreviation-Slang-Replacement/blob/master/finalSlang.csv \n",
        "\n",
        "  # Correct Spelling\n",
        "  # https://github.com/bakwc/JamSpell \n",
        "  # https://github.com/filyp/autocorrect\n",
        "  # https://www.kaggle.com/longtng/nlp-preprocessing-feature-extraction-methods-a-z (tf)\n",
        "  # text_clean_ser = text_clean_ser.apply(lambda x : corrector.FixFragment(x))\n",
        "\n",
        "  # Postprocess Text (lowercase, remove punct/nums, etc)\n",
        "  # text_clean_ser = text_clean_ser.pipe(hero.clean)\n",
        "  # text_clean_ser = text_clean_ser.pipe(hero.clean, hero_post_pipeline)\n",
        "\n",
        "  # pd.Series(tweet_clean_ls)\n",
        "\n",
        "  return text_clean_ser\n",
        "\n",
        "# Test\n",
        "\n",
        "# clean_tweet(tweets_user_df.iloc[:5], 'text')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaDOAAOOoNzU"
      },
      "outputs": [],
      "source": [
        "# tweets_user_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76P3tYBeVVrL"
      },
      "source": [
        "## Get Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PUaTHtdVXaO"
      },
      "outputs": [],
      "source": [
        "# Getting text from a  search string\n",
        "\n",
        "def get_user_tweets(username, count):\n",
        "  '''\n",
        "  Given a username and count\n",
        "  Return {count} number of tweets from {username} Twitter account\n",
        "  '''\n",
        "\n",
        "  tweet_ls = []\n",
        "  tweets_df = pd.DataFrame(columns=['tweet_created_dt','tweet_id','user','acct_desc','loc','following','follwers','total',\n",
        "                                    'user_created_dt','retweet_ct','text','hashtags'])\n",
        "  try:\n",
        "    #creating query methods using parameters\n",
        "    tweets= tweepy.Cursor(api.user_timeline,id= username, tweet_mode=\"extended\").items(count)\n",
        "    tweet_ls = [tweet for tweet in tweets]\n",
        "\n",
        "    for tweet in tweet_ls:   # Pull the values\n",
        "      tweet_id = tweet.id\n",
        "      username = tweet.user.screen_name\n",
        "      acctdesc = tweet.user.description\n",
        "      location = tweet.user.location\n",
        "      following = tweet.user.friends_count\n",
        "      followers = tweet.user.followers_count\n",
        "      totaltweets = tweet.user.statuses_count\n",
        "      usercreatedts = tweet.user.created_at\n",
        "      tweetcreatedts = tweet.created_at\n",
        "      retweetcount = tweet.retweet_count\n",
        "      hashtags = tweet.entities['hashtags']\n",
        "      \n",
        "      try:\n",
        "        text = tweet.retweeted_status.full_text\n",
        "      except AttributeError:     # Not a Retweet\n",
        "        text = tweet.full_text   \n",
        "        \n",
        "      # Add the 11 variables to the empty list - ith_tweet:\n",
        "      ith_tweet = [tweetcreatedts, tweet_id, username, acctdesc, location, following, followers, totaltweets,\n",
        "                    usercreatedts, retweetcount, text, hashtags]  \n",
        "                    \n",
        "      # Append to dataframe - db_tweets\n",
        "      tweets_df.loc[len(tweets_df)] = ith_tweet\n",
        "\n",
        "    #creating dataframe from tweets list\n",
        "    # tweets_df=pd.DataFrame(tweet_ls, columns=['tweet_id','user','acct_desc','loc','following','follwers','total','user_created_dt','tweet_created_dt','retweet_ct','text','hashtags'])\n",
        "  \n",
        "  except BaseException as e:\n",
        "      print('failed on_status', str(e))\n",
        "      time.sleep(3)\n",
        "\n",
        "  return tweets_df\n",
        "\n",
        "# Test\n",
        "\n",
        "get_user_tweets('joebiden',5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbS-XZLfVdzi"
      },
      "outputs": [],
      "source": [
        "def get_matching_tweets(text_query, max_no):\n",
        "  '''\n",
        "  Given a search string and max_no of results\n",
        "  Return a DataFrame of matching results\n",
        "  '''\n",
        "\n",
        "  tweet_ls = []\n",
        "  tweets_df = pd.DataFrame(columns=['tweet_created_dt','tweet_id','user','acct_desc','loc','following','follwers','total',\n",
        "                                    'user_created_dt','retweet_ct','text','hashtags'])\n",
        "\n",
        "  try:\n",
        "    #creation of query method using parameters\n",
        "    tweets= tweepy.Cursor(api.search, q=text_query, tweet_mode=\"extended\", lang='en').items(max_no)\n",
        "\n",
        "    #getting the information from twitter object\n",
        "    tweet_ls= [tweet for tweet in tweets]\n",
        "    #creating a data frame from the list\n",
        "    # tweet_df_from_query= pd.DataFrame(tweet_list, columns=['Date','tweet_id', 'tweet_text'])\n",
        "    # results_df = pd.DataFrame(tweet_list, columns=['Date','tweet_id', 'tweet_text'])\n",
        "\n",
        "    for tweet in tweet_ls:   # Pull the values\n",
        "      tweet_id = tweet.id\n",
        "      username = tweet.user.screen_name\n",
        "      acctdesc = tweet.user.description\n",
        "      location = tweet.user.location\n",
        "      following = tweet.user.friends_count\n",
        "      followers = tweet.user.followers_count\n",
        "      totaltweets = tweet.user.statuses_count\n",
        "      usercreatedts = tweet.user.created_at\n",
        "      tweetcreatedts = tweet.created_at\n",
        "      retweetcount = tweet.retweet_count\n",
        "      hashtags = tweet.entities['hashtags']\n",
        "      \n",
        "      try:\n",
        "        text = tweet.retweeted_status.full_text\n",
        "      except AttributeError:     # Not a Retweet\n",
        "        text = tweet.full_text   \n",
        "        \n",
        "      # Add the 11 variables to the empty list - ith_tweet:\n",
        "      ith_tweet = [tweetcreatedts, tweet_id, username, acctdesc, location, following, followers, totaltweets,\n",
        "                    usercreatedts, retweetcount, text, hashtags]  \n",
        "                    \n",
        "      # Append to dataframe - db_tweets\n",
        "      tweets_df.loc[len(tweets_df)] = ith_tweet\n",
        "\n",
        "    #creating dataframe from tweets list\n",
        "    # tweets_df=pd.DataFrame(tweet_ls, columns=['tweet_created_dt','tweet_id','user','acct_desc','loc','following','follwers','total','user_created_dt','retweet_ct','text','hashtags'])\n",
        "\n",
        "  except BaseException as e:\n",
        "      print('failed_on_status', str(e))\n",
        "      time.sleep(3)\n",
        "\n",
        "  return tweets_df\n",
        "\n",
        "# Test\n",
        "\n",
        "get_matching_tweets('nuggets', 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WSqR-Mi9-S9"
      },
      "source": [
        "# **Get Tweets with Tweepy (Pick only ONE Option: (a) or (b))**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaqF8XuV-B-6"
      },
      "source": [
        "## **Option (a): Read from Saved File**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0LX6TxW-5cw"
      },
      "source": [
        "### **Read Datafile: Query by User**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Q7slODqjmSLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../../2021f_iphs200_programming_humanity/code"
      ],
      "metadata": {
        "id": "i_J6o-CdmnGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTFGC6yF-tQV"
      },
      "outputs": [],
      "source": [
        "!ls *.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6OD1Lln-xmq"
      },
      "outputs": [],
      "source": [
        "# CUSTOMIZE: set to name of users *.csv datafile in directory listed above\n",
        "\n",
        "# Set your tweets.csv datafile name to this variable \n",
        "tweets_user_datafile = 'iphs200_tweets_user.csv'\n",
        "\n",
        "tweets_user_df = pd.read_csv(tweets_user_datafile) # , index_col=[0])\n",
        "tweets_user_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_user_df.shape"
      ],
      "metadata": {
        "id": "SyG2LA7pnFZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_user_df['tweet_created_dt'] = pd.to_datetime(tweets_user_df['tweet_created_dt'])\n",
        "tweets_user_df['user_created_dt'] = pd.to_datetime(tweets_user_df['user_created_dt'])\n",
        "\n",
        "tweets_user_df.info()"
      ],
      "metadata": {
        "id": "d3WZbNg5n_vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate users_ls based upon loaded DataFrame['users'] column\n",
        "\n",
        "users_ls = tweets_user_df['user'].unique()\n",
        "\n",
        "for i, auser in enumerate(users_ls):\n",
        "  print(f'Search Term #{i}: {auser}')\n"
      ],
      "metadata": {
        "id": "6k_2vH4glxHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p2PJRND_L_P"
      },
      "source": [
        "### **Read Datafile: Query by Search Term**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4diERhU_L_Q"
      },
      "outputs": [],
      "source": [
        "!ls *.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Yz1Rml5_L_S"
      },
      "outputs": [],
      "source": [
        "# CUSTOMIZE: set to name of searchterm *.csv datafile in directory listed above\n",
        "tweets_searchterms_datafile = 'iphs200_tweets_query.csv'\n",
        "\n",
        "tweets_searchterm_df = pd.read_csv(tweets_searchterms_datafile) # , index_col=[0])\n",
        "tweets_searchterm_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.shape"
      ],
      "metadata": {
        "id": "-V0a6Bh6qxbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df['tweet_created_dt'] = pd.to_datetime(tweets_searchterm_df['tweet_created_dt'])\n",
        "tweets_searchterm_df['user_created_dt'] = pd.to_datetime(tweets_searchterm_df['user_created_dt'])\n",
        "\n",
        "tweets_searchterm_df.info()"
      ],
      "metadata": {
        "id": "LKcWxQqXqu1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate searchterms_ls based upon loaded DataFrame['users'] column\n",
        "\n",
        "searchterms_ls = tweets_searchterm_df['term'].unique()\n",
        "\n",
        "for i, asearchterm in enumerate(searchterms_ls):\n",
        "  print(f'Search Term #{i}: {asearchterm}')\n"
      ],
      "metadata": {
        "id": "xNQk6Jn-l7Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "**[NOTE] If the tweet text is NOT already cleaned (no text_clean col), \n",
        "then:\n",
        "(a) continue with 'Clean Tweet' section below,\n",
        "else:\n",
        " (b) skip to 'Word Frequency' section below**\n",
        " ```"
      ],
      "metadata": {
        "id": "XAgn437rrA1D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9-GdA5W_Ngp"
      },
      "source": [
        "### **Read Datafile: Query by Trend**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ0zaNlO_Ngr"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncT_y1Jr_Ngs"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "twitter_user_datafile = 'tweets_by_trend_20211106-155252.csv'\n",
        "\n",
        "tweets_by_trend_ = pd.read_csv(twitter_user_datafile)\n",
        "tweets_by_trend_.head()\n",
        "\"\"\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrYdQWig-F7c"
      },
      "source": [
        "## **Option (b): Scrape from Twitter with Tweepy**\n",
        "\n",
        "* https://docs.tweepy.org/en/latest/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hShFU8igCIr"
      },
      "source": [
        "### **Twitter Queries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN2Na03jiqm"
      },
      "source": [
        "#### **Query by User**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYSqHN8mWV3x"
      },
      "source": [
        "**Customize Twitter (a) username list and (b) max_no of tweets to retrieve for each username**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfB6tY4-kH8d"
      },
      "outputs": [],
      "source": [
        "# CUSTOMIZE: add Twitter usernames in the following list\n",
        "\n",
        "users_ls = ['joebiden', 'CDCgov', 'WHO']\n",
        "\n",
        "max_no = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRN8GtCjiOQk"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 33s\n",
        "\n",
        "for i, auser in enumerate(users_ls):\n",
        "  print(f'User #{i}: {auser}')\n",
        "  tweets_user_dt[auser] = get_user_tweets(auser, max_no)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d71YjQ-ZiOKV"
      },
      "outputs": [],
      "source": [
        "# Dictionary of DataFrames, one for each user\n",
        "\n",
        "# View the first few tweets for the first user #0\n",
        "tweets_user_dt[users_ls[0]].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUuDE4R3iOHx"
      },
      "outputs": [],
      "source": [
        "# Merger all User DataFrames into One Master DataFrame: tweets_user_df\n",
        "\n",
        "tweets_user_df = pd.DataFrame()\n",
        "\n",
        "for auser, ausertweets_df in tweets_user_dt.items():\n",
        "  # print(f'key:{auser} - value:{ausertweets_df}')\n",
        "  ausertweets_df['user'] = [auser] * ausertweets_df.shape[0]\n",
        "  tweets_user_df = tweets_user_df.append(ausertweets_df, ignore_index=True)\n",
        "\n",
        "# Sort by Date\n",
        "tweets_user_df.sort_values(by=['tweet_created_dt'], inplace=True) # , ascending=False)\n",
        "tweets_user_df.head()\n",
        "\n",
        "# tweets_user_df.reindex()\n",
        "# tweets_user_df['Date'] = tweets_user_df['Date'].to_datetime()\n",
        "# tweets_user_df = tweets_user_df.set_index('Date')\n",
        "# tweets_user_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwKbBfW03GEJ"
      },
      "outputs": [],
      "source": [
        "# Check for duplicated Tweet_ids\n",
        "\n",
        "tweets_user_df['tweet_id'].duplicated().any()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_user_df.info()"
      ],
      "metadata": {
        "id": "qYctE1okK83G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/43855462/pandas-drop-duplicates-method-not-working-on-dataframe-containing-lists\n",
        "\n",
        "# tweets_user_df.drop_duplicates()\n",
        "tweets_user_df = tweets_user_df.loc[tweets_user_df.astype(str).drop_duplicates().index]\n",
        "tweets_user_df.shape"
      ],
      "metadata": {
        "id": "ELqSA770LSVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTQUDJ9ujmdK"
      },
      "source": [
        "#### **Query by Search Term**\n",
        "\n",
        "TODO: Fix this to mirror the 'Query by User' section above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrZ8GutEh0Pm"
      },
      "outputs": [],
      "source": [
        "# CUSTOMIZE: add Twitter search terms in the following list\n",
        "\n",
        "searchterms_ls = ['vaccines', 'masks', 'mandates', 'travel', 'covid', 'reopening', 'ban']\n",
        "\n",
        "max_no = 15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl5caKx74VGb"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "for i, aterm in enumerate(searchterms_ls):\n",
        "  print(f'Term #{i}: {aterm}')\n",
        "  tweets_searchterm_dt[aterm] = get_matching_tweets(aterm, max_no)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CjgjDsjj7FH"
      },
      "outputs": [],
      "source": [
        "# View the first few tweets for the first searchterm #0\n",
        "\n",
        "tweets_searchterm_dt[searchterms_ls[0]].head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPBqIJW72hso"
      },
      "outputs": [],
      "source": [
        "# Merger all Query DataFrames into One Master DataFrame: tweets_query_all_df\n",
        "\n",
        "tweets_searchterm_df=pd.DataFrame()\n",
        "\n",
        "for aterm, atermtweets_df in tweets_searchterm_dt.items():\n",
        "  atermtweets_df['term'] =[aterm] *atermtweets_df.shape[0]\n",
        "  tweets_searchterm_df = tweets_searchterm_df.append(atermtweets_df, ignore_index=True)\n",
        "\n",
        "# Sort by Date\n",
        "tweets_searchterm_df.sort_values(by=['tweet_created_dt'], inplace=True) # , ascending=False)\n",
        "\n",
        "tweets_searchterm_df.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdP6UkKKMUME"
      },
      "outputs": [],
      "source": [
        "# Check for duplicated Tweet_ids\n",
        "\n",
        "tweets_searchterm_df['tweet_id'].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.info()"
      ],
      "metadata": {
        "id": "QlQ3NjKtMUMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/43855462/pandas-drop-duplicates-method-not-working-on-dataframe-containing-lists\n",
        "\n",
        "# tweets_searchterm_df.drop_duplicates()\n",
        "tweets_searchterm_df = tweets_searchterm_df.loc[~tweets_searchterm_df['tweet_id'].duplicated(keep='last')]\n",
        "# tweets_searchterm_df = tweets_searchterm_df.loc[tweets_searchterm_df.astype(str).drop_duplicates().index]\n",
        "tweets_searchterm_df.shape"
      ],
      "metadata": {
        "id": "C4lontt8MUMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nh9KI9LwE58"
      },
      "outputs": [],
      "source": [
        "tweets_searchterm_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIny6KmE4BQT"
      },
      "source": [
        "#### **Query by Trend**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaLrA-hI4cTW"
      },
      "outputs": [],
      "source": [
        "# Trends\n",
        "\n",
        "tweet_query_trend_df=pd.DataFrame(api.trends_available())\n",
        "# str(trend['country'])\n",
        "# trend['country']=\"United States of America\"\n",
        "tweet_query_trend_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSQDPMKs89eU"
      },
      "source": [
        "### **Clean Tweets by User**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_kSu9OeqPx4"
      },
      "outputs": [],
      "source": [
        "tweets_user_df['text_clean'] = clean_tweet(tweets_user_df, 'text')\n",
        "tweets_user_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DudVMSnpyunn"
      },
      "source": [
        "### Clean Tweets by Search Term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2NvlA6yyrJc"
      },
      "outputs": [],
      "source": [
        "tweets_searchterm_df['text_clean'] = clean_tweet(tweets_searchterm_df, 'text')\n",
        "tweets_searchterm_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM1smOxnxutc"
      },
      "source": [
        "# Word Frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3A0m2nax2QH"
      },
      "source": [
        "## Query by User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdCX3qpIx6QP"
      },
      "outputs": [],
      "source": [
        "# Create pd.Series of most freq top_n words\n",
        " \n",
        "top_n = 10\n",
        "\n",
        "tw_user = hero.visualization.top_words(tweets_user_df['text_clean']).head(top_n)\n",
        "\n",
        "tw_user_df = pd.DataFrame(tw_user)\n",
        "tw_user_df.sort_values('text_clean',inplace=True)\n",
        "tw_user_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ1CEHpBx6QR"
      },
      "outputs": [],
      "source": [
        "# Plot word freq\n",
        "\n",
        "tw_user_df.plot.barh(y='text_clean', legend='None')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title('Word Frequency in Tweets Found by User', fontsize=20)\n",
        "plt.xlabel('Word Count', fontsize=12)\n",
        "plt.ylabel('Word Lemma Root', fontsize=12)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAU5BTPZxxjv"
      },
      "source": [
        "## Query by Search Term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8K6ACIlatJP"
      },
      "outputs": [],
      "source": [
        "# Create pd.Series of most freq top_n words\n",
        " \n",
        "top_n = 10\n",
        "\n",
        "tw_searchterm = hero.visualization.top_words(tweets_searchterm_df['text_clean']).head(10)\n",
        "\n",
        "tw_searchterm_df = pd.DataFrame(tw_searchterm)\n",
        "tw_searchterm_df.sort_values('text_clean',inplace=True)\n",
        "tw_searchterm_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8K5ZvoCuKcI"
      },
      "outputs": [],
      "source": [
        "# Plot word freq\n",
        "\n",
        "tw_searchterm_df.plot.barh(y='text_clean', legend='None')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title('Word Frequency in Tweets Found by Search Term', fontsize=20)\n",
        "plt.xlabel('Word Count', fontsize=12)\n",
        "plt.ylabel('Word Lemma Root', fontsize=12)\n",
        "plt.plot();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A8GwDOYz3k6"
      },
      "outputs": [],
      "source": [
        "type(tw_user_df['text_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FoLnHlMyONI"
      },
      "outputs": [],
      "source": [
        "# TODO: Debug\n",
        "\n",
        "# hero.visualization.top_words(tw_user_df['text_clean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl3j4ea9yMAE"
      },
      "source": [
        "# Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvXkOKLH0SGG"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqwa4-WH0TYs"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2otNYBcyOc6"
      },
      "source": [
        "## Query by User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sp4f1wi0pRZ"
      },
      "outputs": [],
      "source": [
        "# Tokenize all the tweeks and save in list\n",
        "\n",
        "tweets_user_tokens_ls = list(tweets_user_df['text_clean'])\n",
        "# Split each sentence into tokens\n",
        "tweets_user_tokens_ls = [x.split() for x in tweets_user_tokens_ls]\n",
        "# Flatten nested lists\n",
        "tweets_user_tokens_ls = [atoken for sublist in tweets_user_tokens_ls for atoken in sublist]\n",
        "\n",
        "print(tweets_user_tokens_ls)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove selected tokens in black list\n",
        "\n",
        "# CUSTOMIZE: Enter words/tokens to be removed from Word Cloud\n",
        "token_black_ls = ['amp', 'http', 'https']\n",
        "\n",
        "temp_ls = [atoken for atoken in tweets_user_tokens_ls if atoken not in token_black_ls]\n",
        "\n",
        "tweets_user_tokens_ls = temp_ls\n",
        "tweets_user_tokens_ls[:10]"
      ],
      "metadata": {
        "id": "W2TqJSW6O2lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz2_ABjA0sBw"
      },
      "outputs": [],
      "source": [
        "# Generate a word cloud image\n",
        "\n",
        "# Create one big string of tokens\n",
        "tweets_user_str = ' '.join(tweets_user_tokens_ls)\n",
        "\n",
        "# Generate wordcloud\n",
        "wordcloud = WordCloud().generate(tweets_user_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2G0vwqx0wZU"
      },
      "outputs": [],
      "source": [
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edot6REKyQWx"
      },
      "source": [
        "## Query by Search Term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKbR7qsNRRX_"
      },
      "outputs": [],
      "source": [
        "# Tokenize all the tweeks and save in list\n",
        "\n",
        "tweets_searchterm_tokens_ls = list(tweets_searchterm_df['text_clean'])\n",
        "# Split each sentence into tokens\n",
        "tweets_searchterm_tokens_ls = [x.split() for x in tweets_searchterm_tokens_ls]\n",
        "# Flatten nested lists\n",
        "tweets_searchterm_tokens_ls = [atoken for sublist in tweets_searchterm_tokens_ls for atoken in sublist]\n",
        "\n",
        "print(tweets_searchterm_tokens_ls)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove selected tokens in black list\n",
        "\n",
        "# CUSTOMIZE: Enter words/tokens to be removed from Word Cloud\n",
        "token_black_ls = ['\"', 'http', 'https']\n",
        "\n",
        "temp_ls = [atoken for atoken in tweets_searchterm_tokens_ls if atoken not in token_black_ls]\n",
        "\n",
        "tweets_searchterm_tokens_ls = temp_ls\n",
        "tweets_searchterm_tokens_ls[:10]"
      ],
      "metadata": {
        "id": "aZQFVsxQRRYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lne7xHFzRRYD"
      },
      "outputs": [],
      "source": [
        "# Generate a word cloud image\n",
        "\n",
        "# Create one big string of tokens\n",
        "tweets_searchterm_str = ' '.join(tweets_searchterm_tokens_ls)\n",
        "\n",
        "# Generate wordcloud\n",
        "wordcloud = WordCloud().generate(tweets_searchterm_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqU6CFSXRRYE"
      },
      "outputs": [],
      "source": [
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Search Retrieved Tweets for any Phrase**"
      ],
      "metadata": {
        "id": "ofoApD2puBzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query by User"
      ],
      "metadata": {
        "id": "WIl8tOVluIun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_user_df.head(2)"
      ],
      "metadata": {
        "id": "ddsbvTPIvFSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Enter a Text Phrase to Search for in all Retrieved User Tweets:\n",
        "Search_Phrase = \"A Union\" #@param {type:\"string\"}\n",
        "\n",
        "found_user_df = tweets_user_df.loc[tweets_user_df['text_clean'].str.contains(Search_Phrase, case=False)]\n",
        "\n",
        "print(f'{found_user_df.shape[0]} Tweets matched your Search_Phrase: {Search_Phrase}')\n"
      ],
      "metadata": {
        "id": "5Xb3VX_buLMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Matching Tweets\n",
        "\n",
        "found_user_df"
      ],
      "metadata": {
        "id": "jaLQJ_NdvYSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query by Search Term"
      ],
      "metadata": {
        "id": "M8HD-OeduLch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.head()"
      ],
      "metadata": {
        "id": "VXB-rg15uIOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Enter a Text Phrase to Search for in all Retrieved User Tweets:\n",
        "Search_Phrase = \"A Union\" #@param {type:\"string\"}\n",
        "\n",
        "found_searchterm_df = tweets_searchterm_df.loc[tweets_searchterm_df['text_clean'].str.contains(Search_Phrase, case=False)]\n",
        "\n",
        "print(f'{found_searchterm_df.shape[0]} Tweets matched your Search_Phrase: {Search_Phrase}')\n"
      ],
      "metadata": {
        "id": "99DzzXRXvmHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Matching Tweets\n",
        "\n",
        "found_searchterm_df"
      ],
      "metadata": {
        "id": "zGU5F9E6vmHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ols7tsW9vYd"
      },
      "source": [
        "# **Save to File**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query by User"
      ],
      "metadata": {
        "id": "Ec17atE9R7JH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPLx1Px4gFlk"
      },
      "outputs": [],
      "source": [
        "# Save CSV to Google gDrive (if Authenticated, else to temp virtual drive)\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "filename = f'tweets_by_users_{timestr}.csv'\n",
        "\n",
        "tweets_user_df.to_csv(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT3LJWOFnTvw"
      },
      "source": [
        "**!!! REMEMBER TO MANUALLY DOWNLOAD THE DATAFILE JUST CREATED !!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btsWvpve6479"
      },
      "outputs": [],
      "source": [
        "# Save CSV to local laptop\n",
        "\n",
        "files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query by Search Term"
      ],
      "metadata": {
        "id": "t014f9ylSAsn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s1V1X3MR-ty"
      },
      "outputs": [],
      "source": [
        "# Save CSV to Google gDrive (if Authenticated, else to temp virtual drive)\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "filename = f'tweets_by_searchterm_{timestr}.csv'\n",
        "\n",
        "tweets_searchterm_df.to_csv(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_lJX24oR-t0"
      },
      "source": [
        "**!!! REMEMBER TO MANUALLY DOWNLOAD THE DATAFILE JUST CREATED !!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vJ4_muyR-t2"
      },
      "outputs": [],
      "source": [
        "# Save CSV to local laptop\n",
        "\n",
        "files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWR6UrK4qzjG"
      },
      "source": [
        "# **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jFF72vSAhy_"
      },
      "source": [
        "## Query by User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5vduxIjlOFt"
      },
      "outputs": [],
      "source": [
        "# Compute sentiment for each clean_text using VADER\n",
        "\n",
        "# tweets_user_df['vader_all'] = tweets_user_df['text_clean'].apply(lambda x : vader_sa.polarity_scores(x))\n",
        "\n",
        "tweets_user_df['vader'] = tweets_user_df['text_clean'].apply(lambda x : vader_sa.polarity_scores(x)['compound'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw4bxAnIlODL"
      },
      "outputs": [],
      "source": [
        "tweets_user_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knGUrhA72kNQ"
      },
      "outputs": [],
      "source": [
        "tweets_user_df['tweet_id'].duplicated().count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_user_df.drop_duplicates(subset=['tweet_id'], keep='last', inplace=True)\n",
        "tweets_user_df.shape"
      ],
      "metadata": {
        "id": "8xhtqjs-TY2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gquGvQwZr_eN"
      },
      "outputs": [],
      "source": [
        "tweets_user_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_user_df.sort_values(by='tweet_created_dt', inplace=True)\n",
        "tweets_user_df.set_index('tweet_created_dt', inplace=True)\n",
        "tweets_user_df.head(5)"
      ],
      "metadata": {
        "id": "Qw7rF_EVT8j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Raw Plot for ALL Users (Mixed Together)"
      ],
      "metadata": {
        "id": "Ts2GN74UiJDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, auser in enumerate(users_ls):\n",
        "  print(f'User #{i}: {auser}')\n",
        "\n",
        "print(f'\\nPlotting {len(users_ls)} Users')"
      ],
      "metadata": {
        "id": "9oZ5nnGElHE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDoWRajQ4ahU"
      },
      "outputs": [],
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "tweets_user_df['vader'].plot();\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment for all Users', fontsize=20)\n",
        "plt.title(f'{users_ls}', fontsize=14)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=12)\n",
        "plt.show();\n",
        "print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Raw Plot for ALL Users (Separately)"
      ],
      "metadata": {
        "id": "K4TGhgnFo1ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * tweets_user_df.shape[0])\n",
        "\n",
        "for auser in users_ls:\n",
        "  tweets_user_df[tweets_user_df['user'] == auser]['vader'].plot(label=auser)\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment', fontsize=20)\n",
        "plt.title(f'Users: {users_ls}', fontsize=12)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=14)\n",
        "plt.legend(loc='best')\n",
        "plt.show();\n",
        "\n",
        "  # tweets_user_df[tweets_user_df['user'] == auser]['vader'].plot();"
      ],
      "metadata": {
        "id": "uKQx9_bso1I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Raw Plot for a Selected User"
      ],
      "metadata": {
        "id": "wgg1HBhfiaTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, auser in enumerate(users_ls):\n",
        "  print(f'User #{i}: {auser}')\n",
        "\n",
        "print(f'\\nPlotting {len(users_ls)} Users')"
      ],
      "metadata": {
        "id": "Ww8eBoXJlr5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUSTOMIZE: Select one of the usernames from above to plot\n",
        "\n",
        "user_selected = 'sw_columbia'\n",
        "\n",
        "tweets_user_df[tweets_user_df['user'] == user_selected]['vader'].plot()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title(f'Tweet Sentiment \\n User: {user_selected}', fontsize=20)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=12)\n",
        "plt.show();\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "RhDfIcp_itCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Smoothed Plot for ALL Users (Mixed Together)"
      ],
      "metadata": {
        "id": "NIUFYJ3upprg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * tweets_user_df.shape[0])\n",
        "\n",
        "tweets_user_df['vader'].rolling(win_size, center=True, min_periods=0).mean().plot()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment for all Users', fontsize=20)\n",
        "plt.title(f'{users_ls}', fontsize=14)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=12)\n",
        "plt.show();\n",
        "print('\\n')\n"
      ],
      "metadata": {
        "id": "zhqSKGZkppAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Smoothed Plot for ALL Users (Separately)"
      ],
      "metadata": {
        "id": "VHhWzELvqfC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * tweets_user_df.shape[0])\n",
        "\n",
        "for auser in users_ls:\n",
        "  tweets_user_df[tweets_user_df['user'] == auser]['vader'].rolling(win_size, center=True, min_periods=0).mean().plot(label=auser)\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment', fontsize=20)\n",
        "plt.title(f'Users: {users_ls}', fontsize=12)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=14)\n",
        "plt.legend(loc='best')\n",
        "plt.show();\n",
        "\n",
        "  # tweets_user_df[tweets_user_df['user'] == auser]['vader'].plot();"
      ],
      "metadata": {
        "id": "-IIfzlC6qcwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MULTIPLE Smoothed Plot for EACH User"
      ],
      "metadata": {
        "id": "RNxWBgignNrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * tweets_user_df.shape[0])\n",
        "\n",
        "for auser in users_ls:\n",
        "  tweets_user_df[tweets_user_df['user'] == auser]['vader'].rolling(win_size, center=True, min_periods=0).mean().plot()\n",
        "  plt.grid(True, alpha=0.3)\n",
        "  plt.title(f'Tweet Sentiment \\n User: {auser}', fontsize=20)\n",
        "  plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "  plt.ylabel('Sentiment (VADER)', fontsize=14)\n",
        "  plt.show();\n",
        "  print('\\n')\n",
        "  # tweets_user_df[tweets_user_df['user'] == auser]['vader'].plot();"
      ],
      "metadata": {
        "id": "4kKLh2jeiaFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Smoothed Plot for a Selected Users"
      ],
      "metadata": {
        "id": "tUSxmQ0zn3Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, auser in enumerate(users_ls):\n",
        "  print(f'User #{i}: {auser}')\n",
        "\n",
        "print(f'\\nPlotting {len(users_ls)} Users')"
      ],
      "metadata": {
        "id": "fA32k_t0oMTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUSTOMIZE: Select one of the usernames from above to plot\n",
        "\n",
        "user_selected = 'sw_columbia'\n",
        "\n",
        "tweets_user_df[tweets_user_df['user'] == auser]['vader'].rolling(win_size, center=True, min_periods=0).mean().plot()\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment', fontsize=20)\n",
        "plt.title(f'User: {user_selected}', fontsize=20)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=12)\n",
        "plt.show();\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "XkdVJkU9oI4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhfFmFhrAnf1"
      },
      "source": [
        "## Query by Search Term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "740KPwLoAnf7"
      },
      "outputs": [],
      "source": [
        "# Compute sentiment for each clean_text using VADER\n",
        "\n",
        "# tweets_user_all_df['vader_all'] = tweets_user_all_df['text_clean'].apply(lambda x : vader_sa.polarity_scores(x))\n",
        "\n",
        "tweets_searchterm_df['vader'] = tweets_searchterm_df['text_clean'].apply(lambda x : vader_sa.polarity_scores(x)['compound'])\n",
        "tweets_searchterm_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8NekpMPWMO7"
      },
      "outputs": [],
      "source": [
        "tweets_searchterm_df['tweet_id'].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.shape"
      ],
      "metadata": {
        "id": "ZTodPGmtWT8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.drop_duplicates(subset=['tweet_id'], keep='last', inplace=True)\n",
        "tweets_searchterm_df.shape"
      ],
      "metadata": {
        "id": "J-G4KJsYWMO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIZVxvb9WMO-"
      },
      "outputs": [],
      "source": [
        "tweets_searchterm_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.sort_values(by='tweet_created_dt', inplace=True)\n",
        "tweets_searchterm_df.set_index('tweet_created_dt', inplace=True)\n",
        "tweets_searchterm_df.head(5)"
      ],
      "metadata": {
        "id": "6ola7SBSWqnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Raw Plot for ALL Search Terms (Mixed Together)"
      ],
      "metadata": {
        "id": "nRcjI3bBrYU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, asearchterm in enumerate(searchterms_ls):\n",
        "  print(f'Search Term #{i}: {asearchterm}')\n",
        "\n",
        "print(f'\\nPlotting {len(searchterms_ls)} Search Terms')"
      ],
      "metadata": {
        "id": "Dj8mzGqHrYU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.info()"
      ],
      "metadata": {
        "id": "vc74aq2frr0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br3kR4U9rYU2"
      },
      "outputs": [],
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "tweets_searchterm_df['vader'].plot();\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment for all Search Terms', fontsize=20)\n",
        "plt.title(f'{searchterms_ls}', fontsize=14)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=12)\n",
        "plt.show();\n",
        "print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Raw Plot for ALL Search Terms (Separately)"
      ],
      "metadata": {
        "id": "SAcSUW-RrYU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * tweets_searchterm_df.shape[0])\n",
        "\n",
        "for asearchterm in searchterms_ls:\n",
        "  tweets_searchterm_df[tweets_searchterm_df['term'] == asearchterm]['vader'].plot(label=auser)\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment', fontsize=20)\n",
        "plt.title(f'Search Terms: {searchterms_ls}', fontsize=12)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=14)\n",
        "plt.legend(loc='best')\n",
        "plt.show();\n"
      ],
      "metadata": {
        "id": "6ivGKdiOrYU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Raw Plot for a Selected Search Term"
      ],
      "metadata": {
        "id": "nJxBnBBArYU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, auser in enumerate(searchterms_ls):\n",
        "  print(f'User #{i}: {auser}')\n",
        "\n",
        "print(f'\\nPlotting {len(searchterms_ls)} Search Terms')"
      ],
      "metadata": {
        "id": "yWNOwjOerYU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUSTOMIZE: Select one of the usernames from above to plot\n",
        "\n",
        "searchterm_selected = '#CUonStrike'\n",
        "\n",
        "tweets_searchterm_df[tweets_searchterm_df['term'] == searchterm_selected]['vader'].plot()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title(f'Tweet Sentiment \\n User: {searchterm_selected}', fontsize=20)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=12)\n",
        "plt.show();\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "CIF5j550rYU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Smoothed Plot for ALL Search Terms (Mixed Together)"
      ],
      "metadata": {
        "id": "cfRdZfVTrYU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * tweets_searchterm_df.shape[0])\n",
        "\n",
        "tweets_searchterm_df['vader'].rolling(win_size, center=True, min_periods=0).mean().plot()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment for all Search Terms', fontsize=20)\n",
        "plt.title(f'{users_ls}', fontsize=14)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=12)\n",
        "plt.show();\n",
        "print('\\n')\n"
      ],
      "metadata": {
        "id": "nu7REvV9rYU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Smoothed Plot for ALL Search Terms (Separately)"
      ],
      "metadata": {
        "id": "IUexEk8prYVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * tweets_searchterm_df.shape[0])\n",
        "\n",
        "for asearchterm in searchterms_ls:\n",
        "  tweets_searchterm_df[tweets_searchterm_df['term'] == asearchterm]['vader'].rolling(win_size, center=True, min_periods=0).mean().plot(label=asearchterm)\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment', fontsize=20)\n",
        "plt.title(f'Search Terms: {searchterms_ls}', fontsize=12)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=14)\n",
        "plt.legend(loc='best')\n",
        "plt.show();\n",
        "\n",
        "  # tweets_user_df[tweets_user_df['user'] == auser]['vader'].plot();"
      ],
      "metadata": {
        "id": "XS5jTF2XrYVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MULTIPLE Smoothed Plot for EACH Search Term"
      ],
      "metadata": {
        "id": "3W2QT0RgrYVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For a few tweets, plot raw VADER sentiment values\n",
        "\n",
        "win_per = 10\n",
        "win_size = int(win_per/100 * tweets_searchterm_df.shape[0])\n",
        "\n",
        "for asearchterm in searchterms_ls:\n",
        "  tweets_searchterm_df[tweets_searchterm_df['term'] == asearchterm]['vader'].rolling(win_size, center=True, min_periods=0).mean().plot()\n",
        "  plt.grid(True, alpha=0.3)\n",
        "  plt.suptitle(f'Tweet Sentiment', fontsize=20)\n",
        "  plt.title(f'Search Term: {asearchterm}', fontsize=14)\n",
        "  plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "  plt.ylabel('Sentiment (VADER)', fontsize=14)\n",
        "  plt.show();\n",
        "  print('\\n')\n",
        "  # tweets_user_df[tweets_user_df['user'] == auser]['vader'].plot();"
      ],
      "metadata": {
        "id": "lNCvXE2crYVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ONE Smoothed Plot for a Selected Search Term"
      ],
      "metadata": {
        "id": "51KfN5B0rYVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, asearchterm in enumerate(searchterms_ls):\n",
        "  print(f'Search Term #{i}: {asearchterm}')\n",
        "\n",
        "print(f'\\nPlotting {len(searchterms_ls)} Search Terms')"
      ],
      "metadata": {
        "id": "AhD-5KcjrYVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUSTOMIZE: Select one of the usernames from above to plot\n",
        "\n",
        "searchterm_selected = 'union'\n",
        "\n",
        "tweets_searchterm_df[tweets_searchterm_df['term'] == searchterm_selected]['vader'].rolling(win_size, center=True, min_periods=0).mean().plot()\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.suptitle(f'Tweet Sentiment', fontsize=20)\n",
        "plt.title(f'Search Term: {searchterm_selected}', fontsize=14)\n",
        "plt.xlabel('Date Tweet Created', fontsize=14)\n",
        "plt.ylabel('Sentiment (VADER)', fontsize=12)\n",
        "plt.show();\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "WBasxT92rYVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## n Top/Bottom Tweets by Sentiment"
      ],
      "metadata": {
        "id": "PAyVBTJTuiSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query by User"
      ],
      "metadata": {
        "id": "YUHiN7g5u3tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_user_df.info()"
      ],
      "metadata": {
        "id": "vszARsf8uqX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_user_df.iloc[:,[0,1]]"
      ],
      "metadata": {
        "id": "gdFL0nXIymeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown How Many n-top/n-bottom Tweets (ranked by Sentiment):\n",
        "\n",
        "Show_How_Many_Tweets = 22 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "\n",
        "#@markdown Select which Tweet features to Display:\n",
        "# a_created_dt = True #@param {type:\"boolean\"}\n",
        "a_tweet_id = False #@param {type:\"boolean\"}\n",
        "a_user = True #@param {type:\"boolean\"}\n",
        "a_acct_desc = False #@param {type:\"boolean\"}\n",
        "a_loc = False #@param {type:\"boolean\"}\n",
        "a_following = False #@param {type:\"boolean\"}\n",
        "a_followers = False #@param {type:\"boolean\"}\n",
        "a_total = False #@param {type:\"boolean\"}\n",
        "a_user_created_dt = False #@param {type:\"boolean\"}\n",
        "a_retweet_ct = False #@param {type:\"boolean\"}\n",
        "a_text = True #@param {type:\"boolean\"}\n",
        "a_hashtags = False #@param {type:\"boolean\"}\n",
        "a_text_clean = False #@param {type:\"boolean\"}\n",
        "a_vader = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Display in Ascending Sentiment Order?\n",
        "Ascending_Sentiment = True #@param {type:\"boolean\"}\n",
        "\n",
        "cols_selected_ls = []\n",
        "\n",
        "# if a_created_dt:\n",
        "#   cols_selected_ls.append('index.name')\n",
        "if a_tweet_id:\n",
        "  cols_selected_ls.append('tweet_id')\n",
        "if a_user:\n",
        "  cols_selected_ls.append('user')\n",
        "if a_acct_desc:\n",
        "  cols_selected_ls.append('acct_desc')\n",
        "if a_loc:\n",
        "  cols_selected_ls.append('loc')\n",
        "if a_following:\n",
        "  cols_selected_ls.append('following')\n",
        "if a_followers:\n",
        "  cols_selected_ls.append('followers')\n",
        "if a_total:\n",
        "  cols_selected_ls.append('total')\n",
        "if a_user_created_dt:\n",
        "  cols_selected_ls.append('user_created_dt')\n",
        "if a_retweet_ct:\n",
        "  cols_selected_ls.append('retweet_ct')\n",
        "if a_text:\n",
        "  cols_selected_ls.append('text')\n",
        "if a_hashtags:\n",
        "  cols_selected_ls.append('hashtags')\n",
        "if a_text_clean:\n",
        "  cols_selected_ls.append('text_clean')\n",
        "if a_vader:\n",
        "  cols_selected_ls.append('vader')\n",
        "\n",
        "tweets_user_df.sort_values(by='vader', ascending=Ascending_Sentiment).iloc[:Show_How_Many_Tweets][cols_selected_ls]\n"
      ],
      "metadata": {
        "id": "3J4b_zdIvCx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query by Search Term"
      ],
      "metadata": {
        "id": "R37ajsgbu6Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.info()"
      ],
      "metadata": {
        "id": "H-nBOT7vzdq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_searchterm_df.iloc[:,[0,1]]"
      ],
      "metadata": {
        "id": "Gq8amqzWzdq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown How Many n-top/n-bottom Tweets (ranked by Sentiment):\n",
        "\n",
        "Show_How_Many_Tweets = 22 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "\n",
        "#@markdown Select which Tweet features to Display:\n",
        "# a_created_dt = True #@param {type:\"boolean\"}\n",
        "a_tweet_id = False #@param {type:\"boolean\"}\n",
        "a_user = True #@param {type:\"boolean\"}\n",
        "a_acct_desc = False #@param {type:\"boolean\"}\n",
        "a_loc = False #@param {type:\"boolean\"}\n",
        "a_following = False #@param {type:\"boolean\"}\n",
        "a_followers = False #@param {type:\"boolean\"}\n",
        "a_total = False #@param {type:\"boolean\"}\n",
        "a_user_created_dt = False #@param {type:\"boolean\"}\n",
        "a_retweet_ct = False #@param {type:\"boolean\"}\n",
        "a_text = True #@param {type:\"boolean\"}\n",
        "a_hashtags = False #@param {type:\"boolean\"}\n",
        "a_text_clean = False #@param {type:\"boolean\"}\n",
        "a_vader = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Display in Ascending Sentiment Order?\n",
        "Ascending_Sentiment = False #@param {type:\"boolean\"}\n",
        "\n",
        "cols_selected_ls = []\n",
        "\n",
        "# if a_created_dt:\n",
        "#   cols_selected_ls.append('index.name')\n",
        "if a_tweet_id:\n",
        "  cols_selected_ls.append('tweet_id')\n",
        "if a_user:\n",
        "  cols_selected_ls.append('user')\n",
        "if a_acct_desc:\n",
        "  cols_selected_ls.append('acct_desc')\n",
        "if a_loc:\n",
        "  cols_selected_ls.append('loc')\n",
        "if a_following:\n",
        "  cols_selected_ls.append('following')\n",
        "if a_followers:\n",
        "  cols_selected_ls.append('followers')\n",
        "if a_total:\n",
        "  cols_selected_ls.append('total')\n",
        "if a_user_created_dt:\n",
        "  cols_selected_ls.append('user_created_dt')\n",
        "if a_retweet_ct:\n",
        "  cols_selected_ls.append('retweet_ct')\n",
        "if a_text:\n",
        "  cols_selected_ls.append('text')\n",
        "if a_hashtags:\n",
        "  cols_selected_ls.append('hashtags')\n",
        "if a_text_clean:\n",
        "  cols_selected_ls.append('text_clean')\n",
        "if a_vader:\n",
        "  cols_selected_ls.append('vader')\n",
        "\n",
        "tweets_searchterm_df.sort_values(by='vader', ascending=Ascending_Sentiment).iloc[:Show_How_Many_Tweets][cols_selected_ls]\n"
      ],
      "metadata": {
        "id": "9_cMmqOfzdq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **END OF NOTEBOOK**"
      ],
      "metadata": {
        "id": "fY0TiHVrfUrA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "twitter_tweepy_2021214_class.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}